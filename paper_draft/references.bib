@article{tigges2023linear,
  title={Linear Representations of Sentiment in Large Language Models},
  author={Tigges, Curt and Hollinsworth, Oskar John Lincoln and Geiger, Atticus and Nanda, Neel},
  journal={arXiv preprint arXiv:2310.15154},
  year={2023}
}

@inproceedings{peyrard2021laughing,
  title={Laughing Heads: Can Transformers Detect What Makes a Sentence Funny?},
  author={Peyrard, Maxime and Borges, Beatriz and Gligori{\'c}, Kristina and West, Robert},
  booktitle={Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI)},
  year={2021}
}

@article{aghajanyan2020intrinsic,
  title={Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning},
  author={Aghajanyan, Armen and Zettlemoyer, Luke and Gupta, Sonal},
  journal={arXiv preprint arXiv:2012.13255},
  year={2020}
}

@article{hu2021lora,
  title={{LoRA}: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{marks2023geometry,
  title={The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets},
  author={Marks, Samuel and Tegmark, Max},
  journal={arXiv preprint arXiv:2310.06824},
  year={2023}
}

@article{engels2024not,
  title={Not All Language Model Features Are Linear},
  author={Engels, Joshua and Liao, Isaac and Tegmark, Max},
  journal={arXiv preprint arXiv:2405.14860},
  year={2024}
}

@article{wu2024reft,
  title={{ReFT}: Representation Finetuning for Language Models},
  author={Wu, Zhengxuan and Arora, Aryaman and Wang, Zheng and Geiger, Atticus and Jurafsky, Dan and Manning, Christopher D and Potts, Christopher},
  journal={arXiv preprint arXiv:2402.14700},
  year={2024}
}

@article{zou2023representation,
  title={Representation Engineering: A Top-Down Approach to {AI} Transparency},
  author={Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and Goel, Shashwat and Li, Nathaniel and Byun, Michael J and Wang, Zifan and Mallen, Alex and Basart, Steven and Koyber, Sanmi and Song, Dawn and Fredrikson, Matt and Kolter, J Zico and Hendrycks, Dan},
  journal={arXiv preprint arXiv:2310.01405},
  year={2023}
}

@article{hewitt2019designing,
  title={Designing and Interpreting Probes with Control Tasks},
  author={Hewitt, John and Liang, Percy},
  journal={arXiv preprint arXiv:1909.03368},
  year={2019}
}

@article{meaney2021semeval,
  title={{SemEval}-2021 Task 7: {HaHackathon}, Detecting and Rating Humor and Offense},
  author={Meaney, J A and Wilson, Steven R and Chiruzzo, Luis and Lopez, Adam and Magdy, Walid},
  journal={arXiv preprint arXiv:2105.13602},
  year={2021}
}

@article{xie2020uncertainty,
  title={Uncertainty and Surprisal Jointly Deliver the Punchline: Exploiting Incongruity-Based Features for Humor Recognition},
  author={Xie, Yubo and Li, Junze and Pu, Pearl},
  journal={arXiv preprint arXiv:2012.12007},
  year={2020}
}

@inproceedings{weller2019humor,
  title={Humor Detection: A Transformer Gets the Last Laugh},
  author={Weller, Orion and Seppi, Kevin},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2019}
}

@article{burns2022discovering,
  title={Discovering Latent Knowledge in Language Models Without Supervision},
  author={Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2212.03827},
  year={2022}
}

@article{horvitz2024getting,
  title={Getting Serious about Humor: Crafting Humor Datasets with Unfunny {LLM}s},
  author={Horvitz, Zachary and Chen, Jingru and Aditya, Rahul and Srivastava, Kanishk and West, Robert and Yu, Zhou and McKeown, Kathleen},
  journal={arXiv preprint arXiv:2403.00794},
  year={2024}
}

@article{li2022joke,
  title={This Joke is [{MASK}]: Recognizing Humor and Offense with Prompting},
  author={Li, Junze and Zhao, Mengjie and Xie, Yubo and Maronikolakis, Antonis and Pu, Pearl and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2209.12118},
  year={2022}
}

@inproceedings{annamoradnejad2020colbert,
  title={{ColBERT}: Using {BERT} Sentence Embedding for Computational Humor},
  author={Annamoradnejad, Issa and Zoghi, Gohar},
  booktitle={Proceedings of the International Conference on Artificial Intelligence and Statistics},
  year={2020}
}

@article{inacio2023humor,
  title={What do Humor Classifiers Learn? An Attempt to Explain Transformer-based Humor Recognition Models},
  author={In{\'a}cio, Marcio Lima and Wick-Pedro, Gabriela and Gon{\c{c}}alo Oliveira, Hugo},
  journal={Information},
  volume={14},
  number={5},
  pages={300},
  year={2023}
}

@article{choi2023socket,
  title={Do {LLM}s Understand Social Knowledge? Evaluating the Sociability of Large Language Models with the {SocKET} Benchmark},
  author={Choi, Minje and Pei, Jiaxin and Kumar, Sagar and Shu, Chang and Jurgens, David},
  journal={arXiv preprint arXiv:2305.14938},
  year={2023}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI Blog},
  year={2019}
}

@inproceedings{biderman2023pythia,
  title={Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin and Bradley, Herbie and O'Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  booktitle={Proceedings of the 40th International Conference on Machine Learning},
  year={2023}
}

@article{nanda2023emergent,
  title={Emergent Linear Representations in World Models of Self-Supervised Sequence Models},
  author={Nanda, Neel and Lee, Andrew and Wattenberg, Martin},
  journal={arXiv preprint arXiv:2309.00941},
  year={2023}
}

@article{elhage2022toy,
  title={Toy Models of Superposition},
  author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and others},
  journal={arXiv preprint arXiv:2209.10652},
  year={2022}
}
