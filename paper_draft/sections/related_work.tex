\section{Related Work}
\label{sec:related_work}

\para{Linear representations in LLMs.}
The linear representation hypothesis posits that high-level concepts are encoded as directions in a model's residual stream. \citet{tigges2023linear} demonstrated that sentiment is linearly represented in GPT-2 and the Pythia family, with five independent methods (mean difference, K-means, logistic regression, PCA, and DAS) converging to the same direction (cosine similarity 79--99\%). Ablating this direction removes 76\% of sentiment classification accuracy. \citet{marks2023geometry} extended this finding to truth, showing that LLMs form a linear ``truth direction'' that generalizes across diverse factual statement types. \citet{zou2023representation} introduced Representation Engineering, identifying linear directions for honesty, fairness, and harmlessness. More recently, \citet{engels2024not} challenged the universality of this hypothesis by demonstrating that some features (e.g., periodic concepts like days of the week) are represented as multi-dimensional, non-linear structures. Our work extends this line of inquiry to humor, testing whether it behaves as a simple linear feature or a more complex representation.

\para{Computational humor recognition.}
Transformer-based humor detection has achieved near-perfect accuracy on standard benchmarks. \citet{weller2019humor} reported 98.6\% accuracy on the \shortjokes dataset using a fine-tuned BERT model. \citet{meaney2021semeval} introduced the HaHackathon benchmark (SemEval-2021 Task 7), where top systems reached F1 scores of 0.97 for binary humor detection. However, \citet{inacio2023humor} demonstrated that these classifiers rely heavily on stylistic features (punctuation, question words) rather than deep humor understanding---a finding our controlled experiments directly confirm at the representation level. \citet{xie2020uncertainty} modeled humor through incongruity theory using GPT-2 perplexity, suggesting that humor detection may partially reduce to distributional statistics.

\para{Mechanistic analysis of humor in transformers.}
\citet{peyrard2021laughing} provided the first mechanistic analysis of humor processing in transformers, discovering a single ``laughing head'' (attention head 10-6 in BERT) that specializes in attending to humor-bearing tokens. This finding---that humor concentrates in a single model component---is suggestive of low-rank structure, though \citeauthor{peyrard2021laughing} analyzed attention patterns rather than representation dimensionality. \citet{li2022joke} showed that prompting achieves comparable performance to fine-tuning for humor recognition, implying that pre-trained models already contain humor-relevant information. Our work complements these studies by directly measuring the dimensionality of humor in hidden representations rather than in attention patterns or output behavior.

\para{Low-rank structure in fine-tuning.}
\citet{aghajanyan2020intrinsic} showed that NLP fine-tuning tasks have remarkably low intrinsic dimensionality: RoBERTa-Large reaches 90\% of full performance on MRPC with only 200 trainable parameters. Pre-training monotonically decreases intrinsic dimension, and larger models have lower intrinsic dimension. \citet{hu2021lora} exploited this insight to develop \lora, which decomposes weight updates as $\Delta \mW = \mB \mA$ where $\mB \in \R^{d \times r}$ and $\mA \in \R^{r \times k}$ with $r \ll \min(d, k)$. \citet{wu2024reft} pushed this further with LoReFT, achieving competitive results with 10--50$\times$ fewer parameters than \lora by operating directly in representation space. We use \lora rank as an independent measure of humor recognition dimensionality, complementing our probing-based measurements.

\para{Probing methodology.}
\citet{hewitt2019designing} introduced control tasks to validate probing results, ensuring that probe accuracy reflects genuine linguistic knowledge rather than memorization capacity. \citet{burns2022discovering} developed Contrast Consistent Search (CCS) for finding latent knowledge directions without supervision. Our probing methodology follows \citet{tigges2023linear} but extends it with rank-constrained evaluation and controlled confound experiments.
