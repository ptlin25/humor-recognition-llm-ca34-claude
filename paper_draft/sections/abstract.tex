\begin{abstract}
Recent work in mechanistic interpretability has shown that large language models (LLMs) represent concepts like sentiment and truth as linear directions in their activation space. We investigate whether humor recognition shares this property by measuring the effective rank of humor-discriminating subspaces in \gpttwo (124M) and \pythia (410M). Using PCA-constrained linear probes, mean difference directions, and \lora fine-tuning at varying ranks, we find a striking dissociation. When jokes are contrasted with stylistically dissimilar text (factual sentences), humor is essentially \textbf{rank-1}: a single linear direction achieves 99.8\% classification accuracy, and \lora at rank 0 (a classification head alone) achieves 98.3\%. However, when confounds are controlled by contrasting jokes with failed attempts at humor, linear probes achieve only ${\sim}$60\% accuracy regardless of rank---barely above chance. This pattern holds across both models and stands in contrast to sentiment, which reaches 76.7\% under similarly controlled conditions. Our results reveal that what LLMs linearly separate as ``humor'' is primarily text register and style, not humor understanding. True humor quality discrimination appears to require higher-rank or non-linear representations not easily extractable from frozen model activations, challenging the universality of the linear representation hypothesis for complex semantic properties.
\end{abstract}
