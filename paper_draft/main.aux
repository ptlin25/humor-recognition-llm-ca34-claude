\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{tigges2023linear}
\citation{marks2023geometry}
\citation{zou2023representation}
\citation{peyrard2021laughing}
\citation{weller2019humor}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:introduction}{{1}{1}{Introduction}{section.1}{}}
\citation{tigges2023linear}
\citation{marks2023geometry}
\citation{zou2023representation}
\citation{engels2024not}
\citation{weller2019humor}
\citation{meaney2021semeval}
\citation{inacio2023humor}
\citation{xie2020uncertainty}
\citation{peyrard2021laughing}
\citation{peyrard2021laughing}
\citation{li2022joke}
\citation{aghajanyan2020intrinsic}
\citation{hu2021lora}
\citation{wu2024reft}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}\protected@file@percent }
\newlabel{sec:related_work}{{2}{2}{Related Work}{section.2}{}}
\citation{hewitt2019designing}
\citation{burns2022discovering}
\citation{tigges2023linear}
\citation{radford2019language}
\citation{tigges2023linear,marks2023geometry,nanda2023emergent}
\citation{biderman2023pythia}
\citation{tigges2023linear}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Dataset conditions. All conditions are class-balanced. Reddit jokes are filtered to remove \texttt  {[deleted]} posts.\relax }}{3}{table.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:datasets}{{1}{3}{Dataset conditions. All conditions are class-balanced. Reddit jokes are filtered to remove \texttt {[deleted]} posts.\relax }{table.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{3}{section.3}\protected@file@percent }
\newlabel{sec:methodology}{{3}{3}{Methodology}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Models}{3}{subsection.3.1}\protected@file@percent }
\newlabel{sec:models}{{3.1}{3}{Models}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Datasets}{3}{subsection.3.2}\protected@file@percent }
\newlabel{sec:datasets}{{3.2}{3}{Datasets}{subsection.3.2}{}}
\citation{tigges2023linear}
\citation{hu2021lora}
\citation{tigges2023linear}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Activation Extraction}{4}{subsection.3.3}\protected@file@percent }
\newlabel{sec:activation_extraction}{{3.3}{4}{Activation Extraction}{subsection.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Rank-Constrained Linear Probing}{4}{subsection.3.4}\protected@file@percent }
\newlabel{sec:probing}{{3.4}{4}{Rank-Constrained Linear Probing}{subsection.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Mean Difference Probe}{4}{subsection.3.5}\protected@file@percent }
\newlabel{sec:mean_diff}{{3.5}{4}{Mean Difference Probe}{subsection.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}LoRA Fine-Tuning}{4}{subsection.3.6}\protected@file@percent }
\newlabel{sec:lora}{{3.6}{4}{LoRA Fine-Tuning}{subsection.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Evaluation}{4}{subsection.3.7}\protected@file@percent }
\newlabel{sec:evaluation}{{3.7}{4}{Evaluation}{subsection.3.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{4}{section.4}\protected@file@percent }
\newlabel{sec:results}{{4}{4}{Results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Easy Task: Humor Style is Rank-1}{4}{subsection.4.1}\protected@file@percent }
\newlabel{sec:easy_results}{{4.1}{4}{Easy Task: Humor Style is Rank-1}{subsection.4.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Easy task (jokes vs.\ factual): rank-1 probe accuracy by layer for \textsc  {GPT-2}\xspace  . The mean difference and PCA rank-1 probes produce nearly identical results. Bold indicates the best layer.\relax }}{5}{table.caption.3}\protected@file@percent }
\newlabel{tab:easy_probing}{{2}{5}{Easy task (jokes vs.\ factual): rank-1 probe accuracy by layer for \gpttwo . The mean difference and PCA rank-1 probes produce nearly identical results. Bold indicates the best layer.\relax }{table.caption.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Easy task: probe accuracy at varying ranks for \textsc  {GPT-2}\xspace  layer 8. Accuracy saturates at rank 1, confirming that humor-style information is one-dimensional.\relax }}{5}{table.caption.4}\protected@file@percent }
\newlabel{tab:rank_sweep}{{3}{5}{Easy task: probe accuracy at varying ranks for \gpttwo layer 8. Accuracy saturates at rank 1, confirming that humor-style information is one-dimensional.\relax }{table.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}LoRA Fine-Tuning Confirms Low Rank}{5}{subsection.4.2}\protected@file@percent }
\newlabel{sec:lora_results}{{4.2}{5}{LoRA Fine-Tuning Confirms Low Rank}{subsection.4.2}{}}
\citation{tigges2023linear}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Main results for the easy task (jokes vs.\ factual text) using \textsc  {GPT-2}\xspace  . {\em  (Left)}PCA eigenvalue spectrum showing rapid variance decay. {\em  (Right)}Probing accuracy across layers and ranks, confirming rank-1 saturation at intermediate layers.\relax }}{6}{figure.caption.5}\protected@file@percent }
\newlabel{fig:main_results}{{1}{6}{Main results for the easy task (jokes vs.\ factual text) using \gpttwo . \figleft PCA eigenvalue spectrum showing rapid variance decay. \figright Probing accuracy across layers and ranks, confirming rank-1 saturation at intermediate layers.\relax }{figure.caption.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces \textsc  {LoRA}\xspace  fine-tuning results on the easy task with \textsc  {GPT-2}\xspace  . Rank 0 denotes training only the classification head. Accuracy saturates at rank 1--4.\relax }}{6}{table.caption.6}\protected@file@percent }
\newlabel{tab:lora}{{4}{6}{\lora fine-tuning results on the easy task with \gpttwo . Rank 0 denotes training only the classification head. Accuracy saturates at rank 1--4.\relax }{table.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Hard Tasks: Humor Quality is Not Low-Rank}{6}{subsection.4.3}\protected@file@percent }
\newlabel{sec:hard_results}{{4.3}{6}{Hard Tasks: Humor Quality is Not Low-Rank}{subsection.4.3}{}}
\citation{tigges2023linear}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Detailed \textsc  {LoRA}\xspace  rank analysis. {\em  (Left)}Accuracy vs.\ \textsc  {LoRA}\xspace  rank showing early saturation. {\em  (Right)}Trainable parameter count grows linearly with rank, but accuracy gains diminish rapidly after rank 1.\relax }}{7}{figure.caption.7}\protected@file@percent }
\newlabel{fig:lora_detail}{{2}{7}{Detailed \lora rank analysis. \figleft Accuracy vs.\ \lora rank showing early saturation. \figright Trainable parameter count grows linearly with rank, but accuracy gains diminish rapidly after rank 1.\relax }{figure.caption.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Hard task and sentiment results. Controlling for stylistic confounds collapses humor detection accuracy to near-chance. Sentiment remains partially linearly separable. Best results per task in bold.\relax }}{7}{table.caption.8}\protected@file@percent }
\newlabel{tab:hard_tasks}{{5}{7}{Hard task and sentiment results. Controlling for stylistic confounds collapses humor detection accuracy to near-chance. Sentiment remains partially linearly separable. Best results per task in bold.\relax }{table.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Cross-Model Validation: Pythia-410M}{7}{subsection.4.4}\protected@file@percent }
\newlabel{sec:cross_model}{{4.4}{7}{Cross-Model Validation: Pythia-410M}{subsection.4.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{7}{section.5}\protected@file@percent }
\newlabel{sec:discussion}{{5}{7}{Discussion}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}The Humor Direction is a Style Direction}{7}{subsection.5.1}\protected@file@percent }
\newlabel{sec:style_direction}{{5.1}{7}{The Humor Direction is a Style Direction}{subsection.5.1}{}}
\citation{engels2024not}
\citation{hewitt2019designing}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Hard task results. Probe accuracy across layers for the controlled conditions. Unlike the easy task, no layer or rank achieves substantially above-chance accuracy for humor quality discrimination.\relax }}{8}{figure.caption.9}\protected@file@percent }
\newlabel{fig:hard_tasks}{{3}{8}{Hard task results. Probe accuracy across layers for the controlled conditions. Unlike the easy task, no layer or rank achieves substantially above-chance accuracy for humor quality discrimination.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Comparison across conditions. {\em  (Left)}Easy vs.\ hard tasks showing the dramatic accuracy drop when stylistic confounds are controlled. {\em  (Right)}Humor quality vs.\ sentiment, showing that humor quality is less linearly separable than sentiment.\relax }}{8}{figure.caption.10}\protected@file@percent }
\newlabel{fig:comparison}{{4}{8}{Comparison across conditions. \figleft Easy vs.\ hard tasks showing the dramatic accuracy drop when stylistic confounds are controlled. \figright Humor quality vs.\ sentiment, showing that humor quality is less linearly separable than sentiment.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Implications for the Linear Representation Hypothesis}{8}{subsection.5.2}\protected@file@percent }
\newlabel{sec:linear_rep}{{5.2}{8}{Implications for the Linear Representation Hypothesis}{subsection.5.2}{}}
\citation{horvitz2024getting,peyrard2021laughing}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Cross-model validation: easy task results for \textsc  {Pythia-410M}\xspace  . The rank-1 pattern replicates, with perfect accuracy at layer 20.\relax }}{9}{table.caption.11}\protected@file@percent }
\newlabel{tab:pythia}{{6}{9}{Cross-model validation: easy task results for \pythia . The rank-1 pattern replicates, with perfect accuracy at layer 20.\relax }{table.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Cross-model comparison. \textsc  {GPT-2}\xspace  (left) and \textsc  {Pythia-410M}\xspace  (right) show the same pattern on the easy task: rank-1 accuracy peaks in middle-to-late layers, confirming that humor-style detection is a robust, low-rank feature.\relax }}{9}{figure.caption.12}\protected@file@percent }
\newlabel{fig:cross_model}{{5}{9}{Cross-model comparison. \gpttwo (left) and \pythia (right) show the same pattern on the easy task: rank-1 accuracy peaks in middle-to-late layers, confirming that humor-style detection is a robust, low-rank feature.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Implications for Practical Applications}{9}{subsection.5.3}\protected@file@percent }
\newlabel{sec:practical}{{5.3}{9}{Implications for Practical Applications}{subsection.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Limitations}{9}{subsection.5.4}\protected@file@percent }
\newlabel{sec:limitations}{{5.4}{9}{Limitations}{subsection.5.4}{}}
\citation{elhage2022toy}
\citation{peyrard2021laughing,horvitz2024getting}
\bibstyle{plainnat}
\bibdata{references}
\bibcite{aghajanyan2020intrinsic}{{1}{2020}{{Aghajanyan et~al.}}{{Aghajanyan, Zettlemoyer, and Gupta}}}
\bibcite{biderman2023pythia}{{2}{2023}{{Biderman et~al.}}{{Biderman, Schoelkopf, Anthony, Bradley, O'Brien, Hallahan, Khan, Purohit, Prashanth, Raff, et~al.}}}
\bibcite{burns2022discovering}{{3}{2022}{{Burns et~al.}}{{Burns, Ye, Klein, and Steinhardt}}}
\bibcite{elhage2022toy}{{4}{2022}{{Elhage et~al.}}{{Elhage, Hume, Olsson, Schiefer, Henighan, Kravec, Hatfield-Dodds, Lasenby, Drain, Chen, et~al.}}}
\bibcite{engels2024not}{{5}{2024}{{Engels et~al.}}{{Engels, Liao, and Tegmark}}}
\bibcite{hewitt2019designing}{{6}{2019}{{Hewitt and Liang}}{{}}}
\bibcite{horvitz2024getting}{{7}{2024}{{Horvitz et~al.}}{{Horvitz, Chen, Aditya, Srivastava, West, Yu, and McKeown}}}
\bibcite{hu2021lora}{{8}{2021}{{Hu et~al.}}{{Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen}}}
\bibcite{inacio2023humor}{{9}{2023}{{In{\'a}cio et~al.}}{{In{\'a}cio, Wick-Pedro, and Gon{\c {c}}alo~Oliveira}}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{10}{section.6}\protected@file@percent }
\newlabel{sec:conclusion}{{6}{10}{Conclusion}{section.6}{}}
\bibcite{li2022joke}{{10}{2022}{{Li et~al.}}{{Li, Zhao, Xie, Maronikolakis, Pu, and Sch{\"u}tze}}}
\bibcite{marks2023geometry}{{11}{2023}{{Marks and Tegmark}}{{}}}
\bibcite{meaney2021semeval}{{12}{2021}{{Meaney et~al.}}{{Meaney, Wilson, Chiruzzo, Lopez, and Magdy}}}
\bibcite{nanda2023emergent}{{13}{2023}{{Nanda et~al.}}{{Nanda, Lee, and Wattenberg}}}
\bibcite{peyrard2021laughing}{{14}{2021}{{Peyrard et~al.}}{{Peyrard, Borges, Gligori{\'c}, and West}}}
\bibcite{radford2019language}{{15}{2019}{{Radford et~al.}}{{Radford, Wu, Child, Luan, Amodei, and Sutskever}}}
\bibcite{tigges2023linear}{{16}{2023}{{Tigges et~al.}}{{Tigges, Hollinsworth, Geiger, and Nanda}}}
\bibcite{weller2019humor}{{17}{2019}{{Weller and Seppi}}{{}}}
\bibcite{wu2024reft}{{18}{2024}{{Wu et~al.}}{{Wu, Arora, Wang, Geiger, Jurafsky, Manning, and Potts}}}
\bibcite{xie2020uncertainty}{{19}{2020}{{Xie et~al.}}{{Xie, Li, and Pu}}}
\bibcite{zou2023representation}{{20}{2023}{{Zou et~al.}}{{Zou, Phan, Chen, Campbell, Guo, Ren, Pan, Yin, Mazeika, Dombrowski, Goel, Li, Byun, Wang, Mallen, Basart, Koyber, Song, Fredrikson, Kolter, and Hendrycks}}}
\gdef \@abspage@last{11}
